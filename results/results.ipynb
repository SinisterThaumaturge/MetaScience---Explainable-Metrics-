{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from IPython.core.display import display, HTML"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "sys.path.append('./scripts')\n",
    "from evaluate import evaluate_word_level"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "SRC_LANG = 'ro'\n",
    "TGT_LANG = 'en'\n",
    "\n",
    "\n",
    "#for SHAP you need the word_scores they are not included in the pickle\n",
    "SPLIT = 'dev'\n",
    "data_dir = f'data/{SPLIT}/{SRC_LANG}-{TGT_LANG}-{SPLIT}'\n",
    "src = [s.strip() for s in open(f'{data_dir}/{SPLIT}.src').readlines()]\n",
    "tgt = [s.strip() for s in open(f'{data_dir}/{SPLIT}.mt').readlines()]\n",
    "wor = [list(map(int, s.strip().split())) for s in open(f'{data_dir}/{SPLIT}.tgt-tags').readlines()]\n",
    "sen = [float(s.strip()) for s in open(f'{data_dir}/{SPLIT}.da').readlines()]\n",
    "assert len(src) == len(tgt) == len(wor) == len(sen)\n",
    "dataset = {'src': src, 'tgt': tgt, 'word_labels': wor, 'sent_labels': sen}\n",
    "\n",
    "#load results\n",
    "#for De-Zh and Ru-De Deeplift, Occlusion, Integrated Gradients, and LayerXGradientActivation are available\n",
    "lang_pair = SRC_LANG+ '-' +TGT_LANG\n",
    "\n",
    "results_lime_transquest = json.load(open(f\"{lang_pair}/results_lime_complete.json\"))\n",
    "results_lime_xmover = json.load(open(f\"{lang_pair}/results_lime_xmover.json\"))\n",
    "results_deeplift = json.load(open(f\"{lang_pair}/resultsdeeplift.json\"))\n",
    "results_integrated = json.load(open(f\"{lang_pair}/resultsintegratedgradients.json\"))\n",
    "results_layergradientxactivation = json.load(open(f\"{lang_pair}/resultslayerGradientXActivation.json\"))\n",
    "results_occlusion = json.load(open(f\"{lang_pair}/resultsocclusion.json\"))\n",
    "\n",
    "#load results for SHA\n",
    "import pickle\n",
    "results_shap_transquest = pickle.load(open(f\"{lang_pair}/transquest_shap.pkl\",\"rb\"))\n",
    "results_shap_xmover =  pickle.load(open(f\"{lang_pair}/xmover_shap.pkl\",\"rb\"))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#This function evaluates the loaded json files\n",
    "def evaluate_json(result):\n",
    "    evaluations = {}\n",
    "    gold_expls = [item['ground_truth_word'] for item in result]\n",
    "    model_expls = [item['expl'] for item in result]\n",
    "\n",
    "    abs_expls = [np.abs(item) for item in model_expls]\n",
    "    #for lime the explainations are already inverted for all Captum results the result is not inverted\n",
    "    inverted_expls = [np.array(item)*-1 for item in model_expls]\n",
    "    evaluations = {\n",
    "    \"classic\" :  evaluate_word_level(gold_expls, model_expls),\n",
    "    \"abolute\": evaluate_word_level(gold_expls, abs_expls),\n",
    "    \"inverted\" : evaluate_word_level(gold_expls, inverted_expls)\n",
    "    }\n",
    "\n",
    "    #json.dump(evaluations, open(\"evaluations.json\",\"w\"))\n",
    "    return evaluations"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "evaluate_json(results_lime_xmover)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC score: 0.361\n",
      "AP score: 0.299\n",
      "Recall at top-K: 0.192\n",
      "AUC score: 0.401\n",
      "AP score: 0.311\n",
      "Recall at top-K: 0.200\n",
      "AUC score: 0.639\n",
      "AP score: 0.489\n",
      "Recall at top-K: 0.371\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'classic': (0.360895831316344, 0.2992999193070188, 0.19249106359469262),\n",
       " 'abolute': (0.40101096220732363, 0.31060548901259705, 0.20036852182373213),\n",
       " 'inverted': (0.6391041686836568, 0.4888650505925367, 0.3710936258974815)}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "evaluate_json(results_lime_transquest)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC score: 0.413\n",
      "AP score: 0.351\n",
      "Recall at top-K: 0.253\n",
      "AUC score: 0.523\n",
      "AP score: 0.407\n",
      "Recall at top-K: 0.306\n",
      "AUC score: 0.587\n",
      "AP score: 0.501\n",
      "Recall at top-K: 0.395\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'classic': (0.41332733838614183, 0.3511145531505898, 0.252624824715133),\n",
       " 'abolute': (0.5229698480591058, 0.4070380118323182, 0.3060112230741663),\n",
       " 'inverted': (0.5866726616138584, 0.5009950141915821, 0.39460620435132565)}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "#Evaluation for SHAP\n",
    "exp_scores = []\n",
    "abs_exp_scores = []\n",
    "for exp in results_shap_transquest:\n",
    "    scores = [-entry[1] for entry in exp] # use negative SHAP values to find the incorrect tokens\n",
    "    abs_scores = [abs(entry[1]) for entry in exp]\n",
    "    abs_exp_scores.append(abs_scores)\n",
    "    exp_scores.append(scores)\n",
    "    \n",
    "evaluate_word_level(dataset['word_labels'], exp_scores)\n",
    "evaluate_word_level(dataset['word_labels'], abs_exp_scores)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC score: 0.571\n",
      "AP score: 0.487\n",
      "Recall at top-K: 0.376\n",
      "AUC score: 0.582\n",
      "AP score: 0.445\n",
      "Recall at top-K: 0.336\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.581842808121135, 0.44537224608622095, 0.3358871974359703)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "exp_scores = []\n",
    "abs_exp_scores = []\n",
    "for exp in results_shap_xmover:\n",
    "    scores = [-entry[1] for entry in exp] # use negative SHAP values to find the incorrect tokens\n",
    "    abs_scores = [abs(entry[1]) for entry in exp]\n",
    "    abs_exp_scores.append(abs_scores)\n",
    "    exp_scores.append(scores)\n",
    "    \n",
    "evaluate_word_level(dataset['word_labels'], exp_scores)\n",
    "evaluate_word_level(dataset['word_labels'], abs_exp_scores)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC score: 0.583\n",
      "AP score: 0.456\n",
      "Recall at top-K: 0.352\n",
      "AUC score: 0.418\n",
      "AP score: 0.337\n",
      "Recall at top-K: 0.232\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.4181524246098969, 0.3367672131566629, 0.23193147121682445)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "methods = [\"expl_min\", \"expl_max\", \"expl_absmin\",\"expl_absmax\",\"expl_sum\",\"expl_mean\"]\n",
    "ensemble_methods = [\"mean\", \"max\",\"min\"]\n",
    "def ensemble(results,method,wantabs=False):\n",
    "    gold_expls = [item['ground_truth_word'] for item in results_deeplift]\n",
    "    model_expls_mean = []\n",
    "    model_expls_min = []\n",
    "    model_expls_max = []\n",
    "    for items in results:\n",
    "        list_expls = []\n",
    "        max_list_expls = []\n",
    "        min_list_expls = []\n",
    "        for n in items:\n",
    "            if isinstance(n,dict):\n",
    "                try:\n",
    "                    if wantabs:\n",
    "                        list_expls.append(np.abs(np.array(n[method])))\n",
    "                    else:\n",
    "                        list_expls.append(np.array([entry for entry in n[method]]))\n",
    "                except KeyError:\n",
    "                    if wantabs:\n",
    "                        list_expls.append(np.abs(np.array(n[\"expl\"])))\n",
    "                    else:\n",
    "                        list_expls.append(np.array([-entry for entry in n[\"expl\"]]))\n",
    "            else:\n",
    "                if wantabs:\n",
    "                    list_expls.append([abs(entry[1]) for entry in n])\n",
    "                else:\n",
    "                    res = [-entry[1] for entry in n]\n",
    "                    list_expls.append(res)\n",
    "        model_expls_mean.append(list(np.mean(np.array(list_expls),axis=0)))\n",
    "        model_expls_max.append(list(np.max(np.array(list_expls),axis=0)))\n",
    "        model_expls_min.append(list(np.min(np.array(list_expls),axis=0)))\n",
    "    return {\"mean\" : evaluate_word_level(gold_expls, model_expls_mean), \"max\": evaluate_word_level(gold_expls, model_expls_max),\n",
    "    \"min\": evaluate_word_level(gold_expls, model_expls_min) }"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "results_ensemble_abs_without_integrated = ensemble(list(zip(results_lime_transquest,results_shap_xmover)),\"expl_min\",wantabs=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC score: 0.648\n",
      "AP score: 0.565\n",
      "Recall at top-K: 0.462\n",
      "AUC score: 0.648\n",
      "AP score: 0.557\n",
      "Recall at top-K: 0.443\n",
      "AUC score: 0.615\n",
      "AP score: 0.492\n",
      "Recall at top-K: 0.389\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "results_ensemble_abs_without_integrated = ensemble(list(zip(results_occlusion,results_lime_transquest,results_lime_xmover,results_shap_xmover,results_shap_transquest)),\"expl_absmax\",wantabs=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC score: 0.564\n",
      "AP score: 0.412\n",
      "Recall at top-K: 0.288\n",
      "AUC score: 0.596\n",
      "AP score: 0.457\n",
      "Recall at top-K: 0.332\n",
      "AUC score: 0.453\n",
      "AP score: 0.337\n",
      "Recall at top-K: 0.222\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model_expls_mean = []\n",
    "model_expls_min = []\n",
    "model_expls_max = []\n",
    "for i,item in enumerate(results_deeplift):\n",
    "    list_expls = (np.abs(np.array(item[\"expl_absmax\"]))+np.abs(np.array(results_gradientxactivation[i][\"expl_absmax\"]))+np.abs(np.array(results_integrated[i][\"expl_absmax\"])))/3\n",
    "    max_list_expls = np.max(np.abs(np.array([np.array(item[\"expl_absmax\"]),np.array(results_gradientxactivation[i][\"expl_absmax\"]),np.array(results_integrated[i][\"expl_absmax\"])])),axis=0)\n",
    "    min_list_expls =  np.min(np.abs(np.array([np.array(item[\"expl_absmax\"]),np.array(results_gradientxactivation[i][\"expl_absmax\"])])),axis=0)\n",
    "    model_expls_mean.append(list(list_expls))\n",
    "    model_expls_max.append(list(max_list_expls))\n",
    "    model_expls_min.append(list(min_list_expls))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "results_ensemble_abs_without_integrated = ensemble(list(zip(results_occlusion,results_deeplift,results_integrated,results_layergradientxactivation,results_lime_transquest,results_lime_xmover,results_shap_transquest,results_shap_xmover)),\"expl_min\",wantabs=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC score: 0.589\n",
      "AP score: 0.498\n",
      "Recall at top-K: 0.387\n",
      "AUC score: 0.618\n",
      "AP score: 0.495\n",
      "Recall at top-K: 0.381\n",
      "AUC score: 0.494\n",
      "AP score: 0.416\n",
      "Recall at top-K: 0.307\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "results_ensemble_abs_without_integrated = ensemble(list(zip(results_occlusion,results_deeplift,results_integrated,results_layergradientxactivation,results_lime_transquest,results_lime_xmover,results_shap_transquest,results_shap_xmover)),\"expl_absmax\",wantabs=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC score: 0.675\n",
      "AP score: 0.542\n",
      "Recall at top-K: 0.425\n",
      "AUC score: 0.682\n",
      "AP score: 0.546\n",
      "Recall at top-K: 0.429\n",
      "AUC score: 0.461\n",
      "AP score: 0.345\n",
      "Recall at top-K: 0.230\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "#Calculate the runtimes\n",
    "def cal_avg_time(results):\n",
    "    runtime = 0\n",
    "    for item in results:\n",
    "        runtime += item[\"time\"] / len(item[\"expl\"])\n",
    "    return runtime/len(results)*10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "cal_avg_time(results_lime_transquest)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.991070504830115"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "cal_avg_time(results_lime_xmover)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7.058517455793451"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "print(cal_avg_time(results_deeplift))\n",
    "print(cal_avg_time(results_layergradientxactivation))\n",
    "print(cal_avg_time(results_integrated))\n",
    "print(cal_avg_time(results_occlusion))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.12660820017068825\n",
      "0.11603433907003664\n",
      "1.0099605059382362\n",
      "0.9367168408362114\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "\n",
    "#Calculating the All-Zero Baseline\n",
    "gold_expls = [item['ground_truth_word'] for item in results_deeplift]\n",
    "baseline_zero = [list(np.zeros((len(item['ground_truth_word'])))) for item in results_deeplift]\n",
    "baseline_ones =  [list(np.ones((len(item['ground_truth_word'])))) for item in results_deeplift]\n",
    "\n",
    "score = evaluate_word_level(gold_expls, baseline_zero)\n",
    "print(score)\n",
    "score = evaluate_word_level(gold_expls, baseline_ones)\n",
    "print(score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.5, 0.25111811542985535, 0.2105086625884114)\n",
      "(0.5, 0.25111811542985535, 0.2105086625884114)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "interpreter": {
   "hash": "1e44300418bea9fe154b6649c63a84ca2ac6e6076df703d84bbdba8f529e8cfc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}